---
title: "Regularized GBM (xgboost)"
author: "Group 11"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}

if(!require("gbm")){
  install.packages("gbm")
}

if(!require("foreach")){
  install.packages("foreach")
}

if(!require("parallel")){
  install.packages("parallel")
}

if(!require("xgboost")){
  install.packages("xgboost")
}

library("xgboost")
library("EBImage")
library("gbm")
library("foreach")
library("parallel")
```


### Step 0: specify directories.

```{r wkdir, eval=FALSE}
# use relative path for reproducibility
set.seed(2019)
setwd("C:/Users/zengy/Documents/GitHub/Spring2019-Proj3-spring2019-proj3-grp11")
# here replace it with own file path

train_dir <- "../data/train_set/"
train_LR_dir <- paste(train_dir, "LR/", sep="")
train_HR_dir <- paste(train_dir, "HR/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="") 
```

### Step 1: set up controls for evaluation experiments.

In this chunk, we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set
+ (number) K, the number of CV folds
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set
+ (T/F) process features for test set
+ (T/F) run training set

```{r exp_setup}
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
run.train=TRUE # run training set
```

Here we compare the same xGBM with different maximum depth of trees & maximum number of iterations

```{r model_setup}
##### test with different nrounds #####
model_values <- array(NA, dim=c(2, 6))
model_values[1, ]<- rep(c(1,3,5), 2) #values for max_depth
model_values[2, ]<- rep(c(5,20), 3) #values for nrounds

model_values_show = t(model_values)
colnames(model_values_show) = c("max_depth","nrounds")
model_values_show
```

### Step 2: import training images class labels.

We provide extra information of image label: car (0), flower (1), market (2). These labels are not necessary for your model.

```{r train_label}
extra_label <- read.csv(train_label_path, colClasses=c("NULL", NA, NA))
```

### Step 3: construct features and responses

+ `feature_new.R`: extracting features
  + Input: a path for low-resolution images.
  + Input: a path for high-resolution images.
  + Output: an RData file that contains extracted features and corresponding responses

```{r feature}
source("../lib/feature_new.R") #using feature_new

tm_feature_train <- NA
if(run.feature.train){
  tm_feature_train <- system.time(dat_train <- feature(train_LR_dir, train_HR_dir))
  feat_train <- dat_train$feature
  label_train <- dat_train$label
}

save(dat_train, file="../output/feature_train.RData")
```


### Step 4: Train a regression model with training features and responses
Call the train model and test model from library. 

`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. 
+ `train.R`
  + Input: a path that points to the training set features and responses.
  + Output: an RData file that contains trained classifiers in the forms of R objects: models/settings/links to external trained configurations.
+ `test.R`
  + Input: a path that points to the test set features.
  + Input: an R object that contains a trained classifier.
  + Output: an R object of response predictions on the test set. If there are multiple classifiers under evaluation, there should be multiple sets of label predictions. 
```{r loadlib}
source("../lib/train.xgboost.R")
source("../lib/test.xgboost.R")
```

#### Model selection with cross-validation
* Do model selection by choosing among different values of training model parameters, that is, the interaction depth for GBM in this example. 
```{r runcv, message=FALSE, warning=FALSE}
source("../lib/cross_validation_xgboost.R")

if(run.cv){
  err_cv <- array(dim=c(dim(model_values)[2], 2))
  for(k in 1:dim(model_values)[2]){
    cat("Session =", k, "\n")
    cat("max_depth=", model_values[1,k], "\n")
    cat("nrounds=", model_values[2,k], "\n")
    err_cv[k,] <- cv.function(feat_train, label_train, model_values[1,k], model_values[2,k], K)
    cat("Mean CV Error:", err_cv[k,1], "\n")
    cat("Standard Deviation of CV Error:", err_cv[k,2], "\n")
    cat("-----","\n")
  }
  save(err_cv, file="../output/err_cv_xgboost.RData")
}
```

* Since we have experimented with two hyperparameters, we use graphs to visualize hyperparameter choice as below.
```{r cv_vis}
if(run.cv){
  load("../output/err_cv_xgboost.RData")
  plot(c(1,3,5), err_cv[c(4,2,6),1], xlab="Max_Depth", ylab="Mean CV Error",
       main="Cross Validation Error when nrounds = 20", type="n", 
       ylim = c(min(err_cv[c(4,2,6),1])*0.90, max(err_cv[c(4,2,6),1])*1.1))
  points(c(1,3,5), err_cv[c(4,2,6),1], col="blue", pch=16)
  lines(c(1,3,5), err_cv[c(4,2,6),1], col="blue")
  arrows(c(1,3,5), err_cv[c(4,2,6),1]-err_cv[c(4,2,6),2], c(1,3,5), err_cv[c(4,2,6),1]+err_cv[c(4,2,6),2], 
        length=0.1, angle=90, code=3)
  
  points(c(1,3,5), err_cv[c(1,5,3),1], col="red", pch=16)
  lines(c(1,3,5), err_cv[c(1,5,3),1], col="red")
  arrows(c(1,3,5), err_cv[c(1,5,3),1]-err_cv[c(1,5,3),2], c(1,3,5), err_cv[c(1,5,3),1]+err_cv[c(1,5,3),2], 
        length=0.1, angle=90, code=3)
  
  legend("bottomright", legend=c("nrounds = 5","nrounds = 20"), lty = 1, col=c("red","blue"))
}
```

* Choose the "best" parameter values based on mean CV error

```{r best_model}
model_best=model_values[1]
if(run.cv){
  dp_best <- model_values[,which.min(err_cv[,1])][1]
  nr_best <- model_values[,which.min(err_cv[,1])][2]
}

par_best <- list(dp = dp_best, nr = nr_best)
```

* Train the model with the entire training set using the selected model (model parameter) via cross-validation.
```{r final_train}
tm_train=NA
tm_train <- system.time(fit_train <- train(feat_train, label_train, par_best))
save(fit_train, file="../output/fit_train_xgboost.RData")
```

### Step 5: Super-resolution for test images
Feed the final training model with the completely holdout testing data. 
+ `superResolution.R`
  + Input: a path that points to the folder of low-resolution test images.
  + Input: a path that points to the folder (empty) of high-resolution test images.
  + Input: an R object that contains tuned predictors.
  + Output: construct high-resolution versions for each low-resolution test image.
```{r superresolution}
source("../lib/superResolution_new.R") #using superresolution_new
test_dir <- "../data/test_set/" # This will be modified for different data sets.
test_LR_dir <- paste(test_dir, "LR/", sep="")
test_HR_dir <- paste(test_dir, "HR/", sep="")

tm_test=NA
if(run.test){
  load(file="../output/fit_train_xgboost.RData")
  tm_test <- system.time(superResolution(test_LR_dir, test_HR_dir, fit_train))
}
```

### Summarize Running Time
Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. 
```{r running_time}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n")
cat("Time for super-resolution=", tm_test[1], "s \n")
```
